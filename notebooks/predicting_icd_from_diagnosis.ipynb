{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  hadm_id            diagnosis\n",
      "0  142345               SEPSIS\n",
      "1  105331          HEPATITIS B\n",
      "2  165520               SEPSIS\n",
      "3  199207     HUMERAL FRACTURE\n",
      "4  177759  ALCOHOLIC HEPATITIS\n",
      "  hadm_id icd9_code\n",
      "0  142345     99591\n",
      "1  142345     99662\n",
      "2  142345      5672\n",
      "3  142345     40391\n",
      "4  142345     42731\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect(\"MIMIC3_demo.db\")\n",
    "\n",
    "# Load ADMISSIONS and DIAGNOSES_ICD tables\n",
    "admissions_df = pd.read_sql_query(\"SELECT HADM_ID, DIAGNOSIS FROM admissions\", conn)\n",
    "diagnoses_df = pd.read_sql_query(\"SELECT HADM_ID, ICD9_CODE FROM diagnoses_icd\", conn)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Preview the data\n",
    "print(admissions_df.head())\n",
    "print(diagnoses_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  hadm_id diagnosis icd9_code\n",
      "0  142345    SEPSIS     99591\n",
      "1  142345    SEPSIS     99662\n",
      "2  142345    SEPSIS      5672\n",
      "3  142345    SEPSIS     40391\n",
      "4  142345    SEPSIS     42731\n"
     ]
    }
   ],
   "source": [
    "# Merge tables on HADM_ID\n",
    "merged_df = admissions_df.merge(diagnoses_df, on=\"hadm_id\", how=\"inner\")\n",
    "\n",
    "# Preview the merged data\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hadm_id diagnosis icd9_code clean_diagnosis\n",
      "4   142345    SEPSIS     42731          sepsis\n",
      "5   142345    SEPSIS      4280          sepsis\n",
      "15  142345    SEPSIS     25000          sepsis\n",
      "30  165520    SEPSIS       486          sepsis\n",
      "31  165520    SEPSIS     42731          sepsis\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to DIAGNOSIS column\n",
    "merged_df[\"clean_diagnosis\"] = merged_df[\"diagnosis\"].apply(clean_text)\n",
    "\n",
    "# Drop rows with missing or empty text\n",
    "merged_df = merged_df.dropna(subset=[\"clean_diagnosis\"])\n",
    "merged_df = merged_df[merged_df[\"clean_diagnosis\"] != \"\"]\n",
    "\n",
    "# Find the top 10 most frequent ICD codes\n",
    "top_icd_codes = merged_df[\"icd9_code\"].value_counts().head(10).index\n",
    "\n",
    "# Filter the dataset to include only these ICD codes\n",
    "filtered_df = merged_df[merged_df[\"icd9_code\"].isin(top_icd_codes)]\n",
    "\n",
    "# Preview cleaned data\n",
    "print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['25000' '2724' '4019' '42731' '4280' '486' '51881' '5849' '5990' '99592']\n",
      "Encoded Labels Example: [[0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/44/nmc27z_53m30k2b6l0h2c4340000gn/T/ipykernel_81572/3749217610.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df[\"icd_list\"] = filtered_df[\"icd9_code\"].apply(lambda x: [x])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Convert ICD9_CODE into lists (one code per admission in this case)\n",
    "filtered_df[\"icd_list\"] = filtered_df[\"icd9_code\"].apply(lambda x: [x])\n",
    "\n",
    "# Use MultiLabelBinarizer to encode ICD codes\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(filtered_df[\"icd_list\"])\n",
    "\n",
    "# Preview encoded ICD codes\n",
    "print(\"Classes:\", mlb.classes_)\n",
    "print(\"Encoded Labels Example:\", y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 283\n",
      "Test examples: 71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    filtered_df[\"clean_diagnosis\"], y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training examples:\", len(X_train))\n",
    "print(\"Test examples:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ecomak/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "## Augementation \n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import random\n",
    "\n",
    "# Augment text with synonym replacement\n",
    "def synonym_replacement(sentence, n=2):  # Replace `n` words\n",
    "    words = sentence.split()\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        word_to_replace = random.choice(words)\n",
    "        synonyms = wordnet.synsets(word_to_replace)\n",
    "        if synonyms:\n",
    "            synonym = synonyms[0].lemmas()[0].name()\n",
    "            new_words = [synonym if w == word_to_replace else w for w in new_words]\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "# Apply augmentation to training data\n",
    "X_train_augmented = X_train.apply(lambda x: synonym_replacement(x, n=1))\n",
    "\n",
    "# Combine original and augmented data\n",
    "X_train_final = pd.concat([X_train, X_train_augmented])\n",
    "y_train_final = pd.concat([pd.DataFrame(y_train), pd.DataFrame(y_train)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load ClinicalBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "# Tokenize training and test data\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_texts(X_train_final)\n",
    "test_encodings = tokenize_texts(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# Load ClinicalBERT pre-trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "    num_labels=y_train_final.shape[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ecomak/micromamba/envs/clinical_trials/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "\n",
    "# Custom Dataset class\n",
    "class NotesDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = NotesDataset(train_encodings, y_train_final.values)\n",
    "test_dataset = NotesDataset(test_encodings, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/71 [00:00<?, ?it/s]/var/folders/44/nmc27z_53m30k2b6l0h2c4340000gn/T/ipykernel_81572/131402796.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 71/71 [02:43<00:00,  2.31s/it, loss=0.338]\n",
      "Epoch 1: 100%|██████████| 71/71 [03:03<00:00,  2.58s/it, loss=0.299]\n",
      "Epoch 2: 100%|██████████| 71/71 [02:56<00:00,  2.49s/it, loss=0.332]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Train for 3 epochs\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels.float())\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/44/nmc27z_53m30k2b6l0h2c4340000gn/T/ipykernel_81572/131402796.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       25000       0.00      0.00      0.00         3\n",
      "        2724       0.00      0.00      0.00         9\n",
      "        4019       0.00      0.00      0.00        14\n",
      "       42731       0.00      0.00      0.00         7\n",
      "        4280       0.00      0.00      0.00         5\n",
      "         486       0.00      0.00      0.00         4\n",
      "       51881       0.00      0.00      0.00        10\n",
      "        5849       0.00      0.00      0.00         5\n",
      "        5990       0.00      0.00      0.00         8\n",
      "       99592       0.00      0.00      0.00         6\n",
      "\n",
      "   micro avg       0.00      0.00      0.00        71\n",
      "   macro avg       0.00      0.00      0.00        71\n",
      "weighted avg       0.00      0.00      0.00        71\n",
      " samples avg       0.00      0.00      0.00        71\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ecomak/micromamba/envs/clinical_trials/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/ecomak/micromamba/envs/clinical_trials/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/ecomak/micromamba/envs/clinical_trials/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predictions = (torch.sigmoid(logits) > 0.5).cpu().numpy()\n",
    "\n",
    "    y_pred.extend(predictions)\n",
    "    y_true.extend(labels.cpu().numpy())\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred, target_names=mlb.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical_trials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
